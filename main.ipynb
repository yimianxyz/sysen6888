{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwyw5970E5Z6"
      },
      "source": [
        "# Plant Disease Detection\n",
        "### SYSEN6888 - Final Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/6888/project/plant-disease-expert.zip /content\n",
        "!unzip /content/plant-disease-expert.zip"
      ],
      "metadata": {
        "id": "D_duY6FrE-oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Image\\ Data\\ base/Image\\ Data\\ base/algal\\ leaf\\ in\\ tea"
      ],
      "metadata": {
        "id": "Mf3ENE-YKOyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, utils, initializers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "\n",
        "base_path = '/content/Image Data base/Image Data base'\n",
        "\n",
        "\n",
        "BATCH_SIZE = 1000\n",
        "IMG_SIZE = (75, 75)\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "WmiYPsqEKL-2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen, test_gen = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_path,\n",
        "    validation_split=0.15,\n",
        "    subset=\"both\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "\n",
        "VALIDATION_SPLIT = 0.15\n",
        "\n",
        "train_gen = train_gen.take(int((1 - VALIDATION_SPLIT) * len(train_gen)))\n",
        "valid_gen = train_gen.skip(int((1 - VALIDATION_SPLIT) * len(train_gen)))\n",
        "\n",
        "\n",
        "\n",
        "# use cache to improve speed\n",
        "train_gen = train_gen.cache().shuffle(1000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "valid_gen = valid_gen.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_gen = test_gen.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "wfU6OeLOKxaA",
        "outputId": "c2af74e9-d95d-4c2b-db27-ffc3c4bb4ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 199665 files belonging to 58 classes.\n",
            "Using 169716 files for training.\n",
            "Using 29949 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet101, InceptionV3, EfficientNetB1\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IfvQpeRSNU7R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_and_train_model(base_model, train_gen, test_gen, freeze_layers=True):\n",
        "\n",
        "    # Freeze layers\n",
        "    if freeze_layers:\n",
        "      for layer in base_model.layers:\n",
        "          layer.trainable = False\n",
        "\n",
        "    # Add Flatten layer\n",
        "    # x = Flatten()(base_model.output)\n",
        "\n",
        "    # Add GlobalAveragePooling2D layer\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "\n",
        "    # Modify pre-trained model\n",
        "    output = Dense(58, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model for 50 epochs\n",
        "    history = model.fit(train_gen, validation_data=valid_gen, epochs=50)\n",
        "\n",
        "\n",
        "    # Plotting Loss\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()\n",
        "    _, accuracy = model.evaluate(test_gen)\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Get model size (number of parameters)\n",
        "    num_params = model.count_params()\n",
        "\n",
        "    return num_params, inference_time, accuracy"
      ],
      "metadata": {
        "id": "HHBsInrMNVrC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train EfficientNetB1\n",
        "effnet_base = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(75, 75, 3))\n",
        "effnet_params, effnet_time, effnet_acc = create_and_train_model(effnet_base, train_gen, test_gen)\n",
        "print(f\"EfficientNetB1: Params={effnet_params}, Time={effnet_time}, Accuracy={effnet_acc}\")"
      ],
      "metadata": {
        "id": "oIreIAShNkT-",
        "outputId": "4db1c687-4b1c-42dc-bc7d-9fdf0f64f008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}